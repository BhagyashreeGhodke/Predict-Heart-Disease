{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc34c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\" \n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\" \n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\") \n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafd4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, round, rand, sum as spark_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe21293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"HeartDiseasePreprocessingPySpark\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.28\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(\"Spark Session initialized with 8g driver memory and MySQL connector.\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HeartDiseasePreprocessingPySpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.28\") \\\n",
    "    .getOrCreate()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69eb09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_raw_data_path = \"Dataset\"\n",
    "hdfs_output_csv_path = \"Cleaned_dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "859e9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Applies common cleaning and transformation steps to the heart disease dataset.\n",
    "#     Assumes all relevant columns have already been renamed to their standardized forms.\n",
    "#     This function focuses on recoding, filtering invalid values, dropping nulls, and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3853a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def preprocess_heart_data(df):\n",
    "    \n",
    "    final_target_col = \"HeartDiseaseorAttack\"\n",
    "\n",
    "    # --- Recode the target variable (HeartDiseaseorAttack) ---\n",
    "    if final_target_col in df.columns:\n",
    "        df = df.withColumn(final_target_col, \\\n",
    "                            when(col(final_target_col) == 2, lit(0)) \\\n",
    "                            .otherwise(col(final_target_col)))\n",
    "        df = df.filter(~col(final_target_col).isin([7, 9])) # Filter out invalid codes (e.g., Don't know/Refused)\n",
    "    else:\n",
    "        print(f\"Warning: '{final_target_col}' not found in DataFrame. Target processing might be incomplete.\")\n",
    "\n",
    "    # --- Recode other columns based on BRFSS interpretation and filter invalid codes ---\n",
    "    if \"HighBP\" in df.columns:\n",
    "        df = df.withColumn('HighBP', when(col('HighBP') == 1, 1).when(col('HighBP') == 2, 0).otherwise(col('HighBP'))).filter(col('HighBP') != 9)\n",
    "    if \"HighChol\" in df.columns:\n",
    "        df = df.withColumn('HighChol', when(col('HighChol') == 1, 1).when(col('HighChol') == 2, 0).otherwise(col('HighChol'))).filter(~col('HighChol').isin([7, 9]))\n",
    "    if \"CholCheck\" in df.columns:\n",
    "        df = df.withColumn('CholCheck', when(col('CholCheck') == 1, 1).when(col('CholCheck').isin([2, 3]), 0).otherwise(col('CholCheck'))).filter(col('CholCheck') != 9)\n",
    "\n",
    "    if \"BMI\" in df.columns:\n",
    "        df = df.withColumn('BMI', round(col('BMI') / 100, 2))\n",
    "\n",
    "    if \"Smoker\" in df.columns:\n",
    "        df = df.withColumn('Smoker', when(col('Smoker') == 1, 1).when(col('Smoker') == 2, 0).otherwise(col('Smoker'))).filter(~col('Smoker').isin([7, 9]))\n",
    "    if \"Stroke\" in df.columns:\n",
    "        df = df.withColumn('Stroke', when(col('Stroke') == 1, 1).when(col('Stroke') == 2, 0).otherwise(col('Stroke'))).filter(~col('Stroke').isin([7, 9]))\n",
    "    if \"Diabetes\" in df.columns:\n",
    "        df = df.withColumn('Diabetes',\n",
    "                            when(col('Diabetes') == 1, 2)  # Yes\n",
    "                            .when(col('Diabetes') == 2, 0)  # No\n",
    "                            .when(col('Diabetes') == 3, 2)  # Pre-diabetes (grouped with Yes)\n",
    "                            .when(col('Diabetes') == 4, 1)  # Gestational\n",
    "                            .otherwise(col('Diabetes'))).filter(~col('Diabetes').isin([7, 9]))\n",
    "    if \"PhysActivity\" in df.columns:\n",
    "        df = df.withColumn('PhysActivity', when(col('PhysActivity') == 1, 1).when(col('PhysActivity') == 2, 0).otherwise(col('PhysActivity'))).filter(col('PhysActivity') != 9)\n",
    "    if \"Fruits\" in df.columns:\n",
    "        df = df.withColumn('Fruits', when(col('Fruits') == 1, 1).when(col('Fruits') == 2, 0).otherwise(col('Fruits'))).filter(col('Fruits') != 9)\n",
    "    if \"Veggies\" in df.columns:\n",
    "        df = df.withColumn('Veggies', when(col('Veggies') == 1, 1).when(col('Veggies') == 2, 0).otherwise(col('Veggies'))).filter(col('Veggies') != 9)\n",
    "    if \"HvyAlcoholConsump\" in df.columns:\n",
    "        df = df.withColumn('HvyAlcoholConsump', when(col('HvyAlcoholConsump') == 1, 1).when(col('HvyAlcoholConsump') == 2, 0).otherwise(col('HvyAlcoholConsump'))).filter(col('HvyAlcoholConsump') != 9)\n",
    "    if \"AnyHealthcare\" in df.columns:\n",
    "        df = df.withColumn('AnyHealthcare', when(col('AnyHealthcare') == 1, 1).when(col('AnyHealthcare') == 2, 0).otherwise(col('AnyHealthcare'))).filter(~col('AnyHealthcare').isin([7, 9]))\n",
    "    if \"NoDocbcCost\" in df.columns:\n",
    "        df = df.withColumn('NoDocbcCost', when(col('NoDocbcCost') == 1, 1).when(col('NoDocbcCost') == 2, 0).otherwise(col('NoDocbcCost'))).filter(~col('NoDocbcCost').isin([7, 9]))\n",
    "\n",
    "    if \"GenHlth\" in df.columns:\n",
    "        df = df.filter(~col('GenHlth').isin([7, 9]))\n",
    "    if \"MentHlth\" in df.columns:\n",
    "        df = df.withColumn('MentHlth', when(col('MentHlth') == 88, 0).otherwise(col('MentHlth'))).filter(~col('MentHlth').isin([77, 99]))\n",
    "    if \"PhysHlth\" in df.columns:\n",
    "        df = df.withColumn('PhysHlth', when(col('PhysHlth') == 88, 0).otherwise(col('PhysHlth'))).filter(~col('PhysHlth').isin([77, 99]))\n",
    "    if \"DiffWalk\" in df.columns:\n",
    "        df = df.withColumn('DiffWalk', when(col('DiffWalk') == 1, 1).when(col('DiffWalk') == 2, 0).otherwise(col('DiffWalk'))).filter(~col('DiffWalk').isin([7, 9]))\n",
    "    if \"Sex\" in df.columns:\n",
    "        df = df.withColumn('Sex', when(col('Sex') == 1, 1).when(col('Sex') == 2, 0).otherwise(col('Sex'))) # Recode Female to 0, Male to 1\n",
    "    if \"Age\" in df.columns:\n",
    "        df = df.filter(col('Age') != 14)\n",
    "    if \"Education\" in df.columns:\n",
    "        df = df.filter(col('Education') != 9)\n",
    "    if \"Income\" in df.columns:\n",
    "        df = df.filter(~col('Income').isin([77, 99]))\n",
    "\n",
    "    # Define the final set of common columns expected after preprocessing\n",
    "    final_common_columns = [\n",
    "        final_target_col, \"HighBP\", \"HighChol\", \"CholCheck\", \"BMI\",\n",
    "        \"Smoker\", \"Stroke\", \"Diabetes\", \"PhysActivity\", \"Fruits\", \"Veggies\",\n",
    "        \"HvyAlcoholConsump\", \"AnyHealthcare\", \"NoDocbcCost\", \"GenHlth\",\n",
    "        \"MentHlth\", \"PhysHlth\", \"DiffWalk\", \"Sex\", \"Age\", \"Education\", \"Income\"\n",
    "    ]\n",
    "\n",
    "    # Select only the desired columns that are present in the DataFrame\n",
    "    selected_cols = [c for c in final_common_columns if c in df.columns]\n",
    "    df = df.select(*selected_cols)\n",
    "\n",
    "    # Drop rows with any remaining null values\n",
    "    df = df.na.drop()\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "049d07bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 2013 data from HDFS: Dataset/2013.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reading 2013 data from HDFS: {hdfs_raw_data_path}/2013.csv\")\n",
    "df_2013_raw = spark.read.csv(f\"{hdfs_raw_data_path}/2013.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b251ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ALL columns to select and rename for 2013\n",
    "selected_cols_2013_initial = [\n",
    "    'CVDCRHD4', '_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3',\n",
    "    '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV4', 'HLTHPLN1', 'MEDCOST', 'GENHLTH',\n",
    "    'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d19e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2013_renamed = df_2013_raw.select(*selected_cols_2013_initial) \\\n",
    "    .withColumnRenamed('CVDCRHD4', 'HeartDiseaseorAttack') \\\n",
    "    .withColumnRenamed('_RFHYPE5', 'HighBP') \\\n",
    "    .withColumnRenamed('TOLDHI2', 'HighChol') \\\n",
    "    .withColumnRenamed('_CHOLCHK', 'CholCheck') \\\n",
    "    .withColumnRenamed('_BMI5', 'BMI') \\\n",
    "    .withColumnRenamed('SMOKE100', 'Smoker') \\\n",
    "    .withColumnRenamed('CVDSTRK3', 'Stroke') \\\n",
    "    .withColumnRenamed('DIABETE3', 'Diabetes') \\\n",
    "    .withColumnRenamed('_TOTINDA', 'PhysActivity') \\\n",
    "    .withColumnRenamed('_FRTLT1', 'Fruits') \\\n",
    "    .withColumnRenamed('_VEGLT1', 'Veggies') \\\n",
    "    .withColumnRenamed('_RFDRHV4', 'HvyAlcoholConsump') \\\n",
    "    .withColumnRenamed('HLTHPLN1', 'AnyHealthcare') \\\n",
    "    .withColumnRenamed('MEDCOST', 'NoDocbcCost') \\\n",
    "    .withColumnRenamed('GENHLTH', 'GenHlth') \\\n",
    "    .withColumnRenamed('MENTHLTH', 'MentHlth') \\\n",
    "    .withColumnRenamed('PHYSHLTH', 'PhysHlth') \\\n",
    "    .withColumnRenamed('DIFFWALK', 'DiffWalk') \\\n",
    "    .withColumnRenamed('SEX', 'Sex') \\\n",
    "    .withColumnRenamed('_AGEG5YR', 'Age') \\\n",
    "    .withColumnRenamed('EDUCA', 'Education') \\\n",
    "    .withColumnRenamed('INCOME2', 'Income')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef73d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 Processed Row Count: 292441\n"
     ]
    }
   ],
   "source": [
    "df_2013_processed = preprocess_heart_data(df_2013_renamed)\n",
    "print(f\"2013 Processed Row Count: {df_2013_processed.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1f16880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 2015 data from HDFS: Dataset/2015.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reading 2015 data from HDFS: {hdfs_raw_data_path}/2015.csv\")\n",
    "df_2015_raw = spark.read.csv(f\"{hdfs_raw_data_path}/2015.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ba7ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ALL columns to select and rename for 2015\n",
    "selected_cols_2015_initial = [\n",
    "    '_MICHD', '_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3',\n",
    "    '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', 'HLTHPLN1', 'MEDCOST', 'GENHLTH',\n",
    "    'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_renamed = df_2015_raw.select(*selected_cols_2015_initial) \\\n",
    "    .withColumnRenamed('_MICHD', 'HeartDiseaseorAttack') \\\n",
    "    .withColumnRenamed('_RFHYPE5', 'HighBP') \\\n",
    "    .withColumnRenamed('TOLDHI2', 'HighChol') \\\n",
    "    .withColumnRenamed('_CHOLCHK', 'CholCheck') \\\n",
    "    .withColumnRenamed('_BMI5', 'BMI') \\\n",
    "    .withColumnRenamed('SMOKE100', 'Smoker') \\\n",
    "    .withColumnRenamed('CVDSTRK3', 'Stroke') \\\n",
    "    .withColumnRenamed('DIABETE3', 'Diabetes') \\\n",
    "    .withColumnRenamed('_TOTINDA', 'PhysActivity') \\\n",
    "    .withColumnRenamed('_FRTLT1', 'Fruits') \\\n",
    "    .withColumnRenamed('_VEGLT1', 'Veggies') \\\n",
    "    .withColumnRenamed('_RFDRHV5', 'HvyAlcoholConsump') \\\n",
    "    .withColumnRenamed('HLTHPLN1', 'AnyHealthcare') \\\n",
    "    .withColumnRenamed('MEDCOST', 'NoDocbcCost') \\\n",
    "    .withColumnRenamed('GENHLTH', 'GenHlth') \\\n",
    "    .withColumnRenamed('MENTHLTH', 'MentHlth') \\\n",
    "    .withColumnRenamed('PHYSHLTH', 'PhysHlth') \\\n",
    "    .withColumnRenamed('DIFFWALK', 'DiffWalk') \\\n",
    "    .withColumnRenamed('SEX', 'Sex') \\\n",
    "    .withColumnRenamed('_AGEG5YR', 'Age') \\\n",
    "    .withColumnRenamed('EDUCA', 'Education') \\\n",
    "    .withColumnRenamed('INCOME2', 'Income')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04389b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 Processed Row Count: 249137\n"
     ]
    }
   ],
   "source": [
    "df_2015_processed = preprocess_heart_data(df_2015_renamed)\n",
    "print(f\"2015 Processed Row Count: {df_2015_processed.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "251dc658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniting processed dataframes...\n",
      "Cleaned DataFrames merged. Total rows: 541578\n",
      "Schema of merged DataFrame:\n",
      "root\n",
      " |-- HeartDiseaseorAttack: double (nullable = true)\n",
      " |-- HighBP: double (nullable = true)\n",
      " |-- HighChol: double (nullable = true)\n",
      " |-- CholCheck: double (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Smoker: double (nullable = true)\n",
      " |-- Stroke: double (nullable = true)\n",
      " |-- Diabetes: double (nullable = true)\n",
      " |-- PhysActivity: double (nullable = true)\n",
      " |-- Fruits: double (nullable = true)\n",
      " |-- Veggies: double (nullable = true)\n",
      " |-- HvyAlcoholConsump: double (nullable = true)\n",
      " |-- AnyHealthcare: double (nullable = true)\n",
      " |-- NoDocbcCost: double (nullable = true)\n",
      " |-- GenHlth: double (nullable = true)\n",
      " |-- MentHlth: double (nullable = true)\n",
      " |-- PhysHlth: double (nullable = true)\n",
      " |-- DiffWalk: double (nullable = true)\n",
      " |-- Sex: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Education: double (nullable = true)\n",
      " |-- Income: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Uniting processed dataframes...\")\n",
    "merged_df = df_2013_processed.unionByName(df_2015_processed)\n",
    "print(f\"Cleaned DataFrames merged. Total rows: {merged_df.count()}\")\n",
    "print(\"Schema of merged DataFrame:\")\n",
    "merged_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0128450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing class balancing...\n",
      "Original class counts: HeartDiseaseorAttack=0: 498159, HeartDiseaseorAttack=1: 43419\n",
      "Class sizes after balancing:\n",
      "+--------------------+-----+\n",
      "|HeartDiseaseorAttack|count|\n",
      "+--------------------+-----+\n",
      "|                 0.0|43462|\n",
      "|                 1.0|43419|\n",
      "+--------------------+-----+\n",
      "\n",
      "Balanced DataFrame row count: 86881\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing class balancing...\")\n",
    "counts_spark = merged_df.groupBy('HeartDiseaseorAttack').count().collect()\n",
    "majority_count = 0\n",
    "minority_count = 0\n",
    "for row in counts_spark:\n",
    "    if row['HeartDiseaseorAttack'] == 0:\n",
    "        majority_count = row['count']\n",
    "    elif row['HeartDiseaseorAttack'] == 1:\n",
    "        minority_count = row['count']\n",
    "\n",
    "print(f\"Original class counts: HeartDiseaseorAttack=0: {majority_count}, HeartDiseaseorAttack=1: {minority_count}\")\n",
    "\n",
    "df_majority = merged_df.filter(col('HeartDiseaseorAttack') == 0)\n",
    "df_minority = merged_df.filter(col('HeartDiseaseorAttack') == 1)\n",
    "\n",
    "sampling_fraction = minority_count / majority_count if majority_count > 0 else 0\n",
    "\n",
    "df_majority_downsampled = df_majority.sample(False, sampling_fraction, seed=42)\n",
    "\n",
    "balanced_df = df_minority.union(df_majority_downsampled)\n",
    "balanced_df = balanced_df.orderBy(rand(seed=42))\n",
    "\n",
    "print(\"Class sizes after balancing:\")\n",
    "balanced_df.groupBy('HeartDiseaseorAttack').count().show()\n",
    "print(f\"Balanced DataFrame row count: {balanced_df.count()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c23d5160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to dump cleaned dataset to MySQL...\n",
      "Writing data to MySQL table: heart_disease_data_balanced at jdbc:mysql://127.0.0.1:3306/test...\n",
      "Successfully dumped cleaned data to MySQL table: heart_disease_data_balanced\n"
     ]
    }
   ],
   "source": [
    "# Dump Cleaned Dataset to MySQL (retained from previous versions, as requested by user's initial problem context)\n",
    "print(\"Attempting to dump cleaned dataset to MySQL...\")\n",
    "jdbcHostname = \"127.0.0.1\"\n",
    "jdbcPort = 3306\n",
    "jdbcDatabase = \"test\"\n",
    "jdbcUsername = \"bigdata\"\n",
    "jdbcPassword = \"Bigdata@123\"\n",
    "jdbcTableName = \"heart_disease_data_balanced\"\n",
    "\n",
    "jdbcUrl = f\"jdbc:mysql://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?useSSL=false&allowPublicKeyRetrieval=true\"\n",
    "jdbcDriver = \"com.mysql.cj.jdbc.Driver\"\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\" : jdbcUsername,\n",
    "  \"password\" : jdbcPassword,\n",
    "  \"driver\" : jdbcDriver\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(f\"Writing data to MySQL table: {jdbcTableName} at {jdbcUrl.split('?')[0]}...\")\n",
    "    balanced_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbcUrl) \\\n",
    "        .option(\"dbtable\", jdbcTableName) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .options(**connectionProperties) \\\n",
    "        .save()\n",
    "    print(f\"Successfully dumped cleaned data to MySQL table: {jdbcTableName}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to MySQL: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57f72bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving balanced data to HDFS as CSV at: Cleaned_dataset\n",
      "Balanced data saved to HDFS as CSV.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Save the balanced_df to HDFS as a single CSV file for the shell script to copy\n",
    "print(f\"Saving balanced data to HDFS as CSV at: {hdfs_output_csv_path}\")\n",
    "# Coalesce to 1 partition to get a single CSV file, useful for 'hdfs dfs -get'\n",
    "balanced_df.coalesce(1).write.csv(hdfs_output_csv_path, header=True, mode=\"overwrite\")\n",
    "print(\"Balanced data saved to HDFS as CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "584516c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving balanced data directly to local path as CSV at: file:////home/talentum/Heart_Disease_Project/cleaned_heart_data_csv/\n",
      "Balanced data saved directly to local filesystem as CSV.\n",
      "Spark Session stopped.\n"
     ]
    }
   ],
   "source": [
    "# In preprocess_heart_data.py, after class balancing is done:\n",
    "\n",
    "local_output_path = \"file:////home/talentum/Heart_Disease_Project/cleaned_heart_data_csv/\" # Note the three slashes for an absolute path\n",
    "\n",
    "print(f\"Saving balanced data directly to local path as CSV at: {local_output_path}\")\n",
    "\n",
    "# Coalesce to 1 partition to ensure a single CSV file is created in the output directory.\n",
    "# 'header=True' includes column names, 'mode=\"overwrite\"' handles existing files.\n",
    "balanced_df.coalesce(1).write.csv(local_output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Balanced data saved directly to local filesystem as CSV.\")\n",
    "\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n",
    "print(\"Spark Session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdf758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
