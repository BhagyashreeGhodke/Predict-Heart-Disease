{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6290a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session initialized with 8g driver memory and MySQL connector.\n",
      "Loading and preprocessing 2013 data...\n",
      "Processed 2013 data saved to Dataset/tmp/heart_disease_2013_processed.parquet\n",
      "2013 Processed Row Count: 292441\n",
      "Loading and preprocessing 2015 data...\n",
      "Processed 2015 data saved to Dataset/tmp/heart_disease_2015_processed.parquet\n",
      "2015 Processed Row Count: 249137\n",
      "Loading processed data from Parquet and uniting...\n",
      "Cleaned DataFrames merged. Total rows: 541578\n",
      "Schema of merged DataFrame:\n",
      "root\n",
      " |-- HeartDiseaseorAttack: double (nullable = true)\n",
      " |-- HighBP: double (nullable = true)\n",
      " |-- HighChol: double (nullable = true)\n",
      " |-- CholCheck: double (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Smoker: double (nullable = true)\n",
      " |-- Stroke: double (nullable = true)\n",
      " |-- Diabetes: double (nullable = true)\n",
      " |-- PhysActivity: double (nullable = true)\n",
      " |-- Fruits: double (nullable = true)\n",
      " |-- Veggies: double (nullable = true)\n",
      " |-- HvyAlcoholConsump: double (nullable = true)\n",
      " |-- AnyHealthcare: double (nullable = true)\n",
      " |-- NoDocbcCost: double (nullable = true)\n",
      " |-- GenHlth: double (nullable = true)\n",
      " |-- MentHlth: double (nullable = true)\n",
      " |-- PhysHlth: double (nullable = true)\n",
      " |-- DiffWalk: double (nullable = true)\n",
      " |-- Sex: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Education: double (nullable = true)\n",
      " |-- Income: double (nullable = true)\n",
      "\n",
      "Performing class balancing...\n",
      "Original class counts: HeartDiseaseorAttack=0: 498159, HeartDiseaseorAttack=1: 43419\n",
      "Class sizes after balancing:\n",
      "+--------------------+-----+\n",
      "|HeartDiseaseorAttack|count|\n",
      "+--------------------+-----+\n",
      "|                 0.0|43498|\n",
      "|                 1.0|43419|\n",
      "+--------------------+-----+\n",
      "\n",
      "Balanced DataFrame row count: 86917\n",
      "Attempting to dump cleaned dataset to MySQL...\n",
      "Writing data to MySQL table: heart_disease_data_balanced at jdbc:mysql://127.0.0.1:3306/test...\n",
      "Successfully dumped cleaned data to MySQL table: heart_disease_data_balanced\n",
      "Spark Session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initialization and Spark Session Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set environment variables for Spark (adjust paths if necessary for your environment)\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\" # Adjust as per your Spark installation\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" # Ensure this matches your Python version\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\" # Ensure this matches your Python version\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\") # Adjust py4j version if needed\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")\n",
    "\n",
    "# Ensure MySQL connector package is included for Spark submit.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages mysql:mysql-connector-java:8.0.28 pyspark-shell'\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, round, rand, sum as spark_sum\n",
    "\n",
    "# Initialize Spark Session with increased driver memory and MySQL connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HeartDiseasePreprocessingPySpark\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.28\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session initialized with 8g driver memory and MySQL connector.\")\n",
    "\n",
    "# Cell 2: Define Preprocessing Function\n",
    "def preprocess_heart_data(df):\n",
    "    \"\"\"\n",
    "    Applies common cleaning and transformation steps to the heart disease dataset.\n",
    "    Assumes all relevant columns have already been renamed to their standardized forms.\n",
    "    This function focuses on recoding, filtering invalid values, dropping nulls, and duplicates.\n",
    "    \"\"\"\n",
    "    final_target_col = \"HeartDiseaseorAttack\"\n",
    "\n",
    "    # --- Recode the target variable (Heart Disease/Attack) ---\n",
    "    if final_target_col in df.columns:\n",
    "        df = df.withColumn(final_target_col, \\\n",
    "                            when(col(final_target_col) == 2, lit(0)) \\\n",
    "                            .otherwise(col(final_target_col)))\n",
    "        df = df.filter(~col(final_target_col).isin([7, 9])) # Filter out invalid codes (e.g., Don't know/Refused)\n",
    "    else:\n",
    "        # This case should ideally not happen if renaming in Cells 3 & 4 is correct\n",
    "        print(f\"Warning: '{final_target_col}' not found in DataFrame. Target processing might be incomplete.\")\n",
    "\n",
    "    # --- Recode other columns based on BRFSS interpretation and filter invalid codes ---\n",
    "    # HighBP: 1=Yes, 2=No -> 0=No, 1=Yes (filter 9=Don't know/Refused)\n",
    "    if \"HighBP\" in df.columns:\n",
    "        df = df.withColumn('HighBP', when(col('HighBP') == 1, 1).when(col('HighBP') == 2, 0).otherwise(col('HighBP'))).filter(col('HighBP') != 9)\n",
    "    # HighChol: 1=Yes, 2=No -> 0=No, 1=Yes (filter 7=Don't know, 9=Refused)\n",
    "    if \"HighChol\" in df.columns:\n",
    "        df = df.withColumn('HighChol', when(col('HighChol') == 1, 1).when(col('HighChol') == 2, 0).otherwise(col('HighChol'))).filter(~col('HighChol').isin([7, 9]))\n",
    "    # CholCheck: 1=Yes, 2=No, 3=Never -> 0=No, 1=Yes (filter 9=Don't know/Refused)\n",
    "    if \"CholCheck\" in df.columns:\n",
    "        df = df.withColumn('CholCheck', when(col('CholCheck') == 1, 1).when(col('CholCheck').isin([2, 3]), 0).otherwise(col('CholCheck'))).filter(col('CholCheck') != 9)\n",
    "    \n",
    "    # BMI: Already numeric, just round\n",
    "    if \"BMI\" in df.columns:\n",
    "        df = df.withColumn('BMI', round(col('BMI'), 0)) # Assuming _BMI5 / 100 already happened during rename/initial processing\n",
    "\n",
    "    # Smoker: 1=Yes, 2=No -> 0=No, 1=Yes (filter 7, 9)\n",
    "    if \"Smoker\" in df.columns:\n",
    "        df = df.withColumn('Smoker', when(col('Smoker') == 1, 1).when(col('Smoker') == 2, 0).otherwise(col('Smoker'))).filter(~col('Smoker').isin([7, 9]))\n",
    "    # Stroke: 1=Yes, 2=No -> 0=No, 1=Yes (filter 7, 9)\n",
    "    if \"Stroke\" in df.columns:\n",
    "        df = df.withColumn('Stroke', when(col('Stroke') == 1, 1).when(col('Stroke') == 2, 0).otherwise(col('Stroke'))).filter(~col('Stroke').isin([7, 9]))\n",
    "    # Diabetes: 1=Yes, 2=No, 3=Pre-diabetes, 4=Gestational -> 0=No, 1=Gestational, 2=Yes/Pre-diabetes (filter 7, 9)\n",
    "    if \"Diabetes\" in df.columns:\n",
    "        df = df.withColumn('Diabetes',\n",
    "                            when(col('Diabetes') == 1, 2)  # Yes\n",
    "                            .when(col('Diabetes') == 2, 0)  # No\n",
    "                            .when(col('Diabetes') == 3, 2)  # Pre-diabetes (grouped with Yes)\n",
    "                            .when(col('Diabetes') == 4, 1)  # Gestational\n",
    "                            .otherwise(col('Diabetes'))).filter(~col('Diabetes').isin([7, 9]))\n",
    "    # PhysActivity: 1=Yes, 2=No -> 0=No, 1=Yes (filter 9)\n",
    "    if \"PhysActivity\" in df.columns:\n",
    "        df = df.withColumn('PhysActivity', when(col('PhysActivity') == 1, 1).when(col('PhysActivity') == 2, 0).otherwise(col('PhysActivity'))).filter(col('PhysActivity') != 9)\n",
    "    # Fruits: 1=Yes, 2=No -> 0=No, 1=Yes (filter 9)\n",
    "    if \"Fruits\" in df.columns:\n",
    "        df = df.withColumn('Fruits', when(col('Fruits') == 1, 1).when(col('Fruits') == 2, 0).otherwise(col('Fruits'))).filter(col('Fruits') != 9)\n",
    "    # Veggies: 1=Yes, 2=No -> 0=No, 1=Yes (filter 9)\n",
    "    if \"Veggies\" in df.columns:\n",
    "        df = df.withColumn('Veggies', when(col('Veggies') == 1, 1).when(col('Veggies') == 2, 0).otherwise(col('Veggies'))).filter(col('Veggies') != 9)\n",
    "    # HvyAlcoholConsump: 1=Yes, 2=No -> 0=No, 1=Yes (filter 9)\n",
    "    if \"HvyAlcoholConsump\" in df.columns:\n",
    "        df = df.withColumn('HvyAlcoholConsump', when(col('HvyAlcoholConsump') == 1, 1).when(col('HvyAlcoholConsump') == 2, 0).otherwise(col('HvyAlcoholConsump'))).filter(col('HvyAlcoholConsump') != 9)\n",
    "    # AnyHealthcare: 1=Yes, 2=No -> 0=No, 1=Yes (filter 7, 9)\n",
    "    if \"AnyHealthcare\" in df.columns:\n",
    "        df = df.withColumn('AnyHealthcare', when(col('AnyHealthcare') == 1, 1).when(col('AnyHealthcare') == 2, 0).otherwise(col('AnyHealthcare'))).filter(~col('AnyHealthcare').isin([7, 9]))\n",
    "    # NoDocbcCost: 1=Yes, 2=No -> 0=No, 1=Yes (filter 7, 9)\n",
    "    if \"NoDocbcCost\" in df.columns:\n",
    "        df = df.withColumn('NoDocbcCost', when(col('NoDocbcCost') == 1, 1).when(col('NoDocbcCost') == 2, 0).otherwise(col('NoDocbcCost'))).filter(~col('NoDocbcCost').isin([7, 9]))\n",
    "    \n",
    "    # GenHlth: 1-5 scale (filter 7, 9)\n",
    "    if \"GenHlth\" in df.columns:\n",
    "        df = df.filter(~col('GenHlth').isin([7, 9]))\n",
    "    # MentHlth: 1-30 days, 88=None -> 0=None, else days (filter 77, 99)\n",
    "    if \"MentHlth\" in df.columns:\n",
    "        df = df.withColumn('MentHlth', when(col('MentHlth') == 88, 0).otherwise(col('MentHlth'))).filter(~col('MentHlth').isin([77, 99]))\n",
    "    # PhysHlth: 1-30 days, 88=None -> 0=None, else days (filter 77, 99)\n",
    "    if \"PhysHlth\" in df.columns:\n",
    "        df = df.withColumn('PhysHlth', when(col('PhysHlth') == 88, 0).otherwise(col('PhysHlth'))).filter(~col('PhysHlth').isin([77, 99]))\n",
    "    # DiffWalk: 1=Yes, 2=No -> 0=No, 1=Yes (filter 7, 9)\n",
    "    if \"DiffWalk\" in df.columns:\n",
    "        df = df.withColumn('DiffWalk', when(col('DiffWalk') == 1, 1).when(col('DiffWalk') == 2, 0).otherwise(col('DiffWalk'))).filter(~col('DiffWalk').isin([7, 9]))\n",
    "    # Sex: 1=Male, 2=Female -> 0=Female, 1=Male\n",
    "    if \"Sex\" in df.columns:\n",
    "        df = df.withColumn('Sex', when(col('Sex') == 1, 1).when(col('Sex') == 2, 0).otherwise(col('Sex'))) # Recode Female to 0, Male to 1\n",
    "    # Age: 1-13 (age groups), filter 14 (Don't know/Refused)\n",
    "    if \"Age\" in df.columns:\n",
    "        df = df.filter(col('Age') != 14)\n",
    "    # Education: 1-6 (filter 9)\n",
    "    if \"Education\" in df.columns:\n",
    "        df = df.filter(col('Education') != 9)\n",
    "    # Income: 1-8 (filter 77, 99)\n",
    "    if \"Income\" in df.columns:\n",
    "        df = df.filter(~col('Income').isin([77, 99]))\n",
    "\n",
    "    # Define the final set of common columns expected after preprocessing\n",
    "    final_common_columns = [\n",
    "        final_target_col, \"HighBP\", \"HighChol\", \"CholCheck\", \"BMI\",\n",
    "        \"Smoker\", \"Stroke\", \"Diabetes\", \"PhysActivity\", \"Fruits\", \"Veggies\",\n",
    "        \"HvyAlcoholConsump\", \"AnyHealthcare\", \"NoDocbcCost\", \"GenHlth\",\n",
    "        \"MentHlth\", \"PhysHlth\", \"DiffWalk\", \"Sex\", \"Age\", \"Education\", \"Income\"\n",
    "    ]\n",
    "\n",
    "    # Select only the desired columns that are present in the DataFrame\n",
    "    # This ensures consistency in column order and presence before union\n",
    "    selected_cols = [c for c in final_common_columns if c in df.columns]\n",
    "    df = df.select(*selected_cols)\n",
    "\n",
    "    # Drop rows with any remaining null values across selected columns\n",
    "    df = df.na.drop()\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Cell 3: Load 2013 CSV Data, Rename Columns, and Preprocess\n",
    "print(\"Loading and preprocessing 2013 data...\")\n",
    "df_2013_raw = spark.read.csv('Dataset/2013.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Define ALL columns to select and rename for 2013 BEFORE calling preprocess_heart_data\n",
    "# Importantly, CVDCRHD4 is now selected and renamed to HeartDiseaseorAttack here.\n",
    "selected_cols_2013_initial = [\n",
    "    'CVDCRHD4', '_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3',\n",
    "    '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV4', 'HLTHPLN1', 'MEDCOST', 'GENHLTH',\n",
    "    'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2'\n",
    "]\n",
    "\n",
    "df_2013_renamed = df_2013_raw.select(*selected_cols_2013_initial) \\\n",
    "    .withColumnRenamed('CVDCRHD4', 'HeartDiseaseorAttack') \\\n",
    "    .withColumnRenamed('_RFHYPE5', 'HighBP') \\\n",
    "    .withColumnRenamed('TOLDHI2', 'HighChol') \\\n",
    "    .withColumnRenamed('_CHOLCHK', 'CholCheck') \\\n",
    "    .withColumnRenamed('_BMI5', 'BMI') \\\n",
    "    .withColumnRenamed('SMOKE100', 'Smoker') \\\n",
    "    .withColumnRenamed('CVDSTRK3', 'Stroke') \\\n",
    "    .withColumnRenamed('DIABETE3', 'Diabetes') \\\n",
    "    .withColumnRenamed('_TOTINDA', 'PhysActivity') \\\n",
    "    .withColumnRenamed('_FRTLT1', 'Fruits') \\\n",
    "    .withColumnRenamed('_VEGLT1', 'Veggies') \\\n",
    "    .withColumnRenamed('_RFDRHV4', 'HvyAlcoholConsump') \\\n",
    "    .withColumnRenamed('HLTHPLN1', 'AnyHealthcare') \\\n",
    "    .withColumnRenamed('MEDCOST', 'NoDocbcCost') \\\n",
    "    .withColumnRenamed('GENHLTH', 'GenHlth') \\\n",
    "    .withColumnRenamed('MENTHLTH', 'MentHlth') \\\n",
    "    .withColumnRenamed('PHYSHLTH', 'PhysHlth') \\\n",
    "    .withColumnRenamed('DIFFWALK', 'DiffWalk') \\\n",
    "    .withColumnRenamed('SEX', 'Sex') \\\n",
    "    .withColumnRenamed('_AGEG5YR', 'Age') \\\n",
    "    .withColumnRenamed('EDUCA', 'Education') \\\n",
    "    .withColumnRenamed('INCOME2', 'Income')\n",
    "\n",
    "df_2013_processed = preprocess_heart_data(df_2013_renamed)\n",
    "\n",
    "output_path_2013 = \"Dataset/tmp/heart_disease_2013_processed.parquet\"\n",
    "df_2013_processed.write.mode(\"overwrite\").parquet(output_path_2013)\n",
    "print(f\"Processed 2013 data saved to {output_path_2013}\")\n",
    "print(f\"2013 Processed Row Count: {df_2013_processed.count()}\")\n",
    "\n",
    "\n",
    "# Cell 4: Load 2015 CSV Data, Rename Columns, and Preprocess\n",
    "print(\"Loading and preprocessing 2015 data...\")\n",
    "df_2015_raw = spark.read.csv('Dataset/2015.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Define ALL columns to select and rename for 2015 BEFORE calling preprocess_heart_data\n",
    "# Importantly, _MICHD is now selected and renamed to HeartDiseaseorAttack here.\n",
    "selected_cols_2015_initial = [\n",
    "    '_MICHD', '_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3',\n",
    "    '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', 'HLTHPLN1', 'MEDCOST', 'GENHLTH',\n",
    "    'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2'\n",
    "]\n",
    "\n",
    "df_2015_renamed = df_2015_raw.select(*selected_cols_2015_initial) \\\n",
    "    .withColumnRenamed('_MICHD', 'HeartDiseaseorAttack') \\\n",
    "    .withColumnRenamed('_RFHYPE5', 'HighBP') \\\n",
    "    .withColumnRenamed('TOLDHI2', 'HighChol') \\\n",
    "    .withColumnRenamed('_CHOLCHK', 'CholCheck') \\\n",
    "    .withColumnRenamed('_BMI5', 'BMI') \\\n",
    "    .withColumnRenamed('SMOKE100', 'Smoker') \\\n",
    "    .withColumnRenamed('CVDSTRK3', 'Stroke') \\\n",
    "    .withColumnRenamed('DIABETE3', 'Diabetes') \\\n",
    "    .withColumnRenamed('_TOTINDA', 'PhysActivity') \\\n",
    "    .withColumnRenamed('_FRTLT1', 'Fruits') \\\n",
    "    .withColumnRenamed('_VEGLT1', 'Veggies') \\\n",
    "    .withColumnRenamed('_RFDRHV5', 'HvyAlcoholConsump') \\\n",
    "    .withColumnRenamed('HLTHPLN1', 'AnyHealthcare') \\\n",
    "    .withColumnRenamed('MEDCOST', 'NoDocbcCost') \\\n",
    "    .withColumnRenamed('GENHLTH', 'GenHlth') \\\n",
    "    .withColumnRenamed('MENTHLTH', 'MentHlth') \\\n",
    "    .withColumnRenamed('PHYSHLTH', 'PhysHlth') \\\n",
    "    .withColumnRenamed('DIFFWALK', 'DiffWalk') \\\n",
    "    .withColumnRenamed('SEX', 'Sex') \\\n",
    "    .withColumnRenamed('_AGEG5YR', 'Age') \\\n",
    "    .withColumnRenamed('EDUCA', 'Education') \\\n",
    "    .withColumnRenamed('INCOME2', 'Income')\n",
    "\n",
    "df_2015_processed = preprocess_heart_data(df_2015_renamed)\n",
    "\n",
    "output_path_2015 = \"Dataset/tmp/heart_disease_2015_processed.parquet\"\n",
    "df_2015_processed.write.mode(\"overwrite\").parquet(output_path_2015)\n",
    "print(f\"Processed 2015 data saved to {output_path_2015}\")\n",
    "print(f\"2015 Processed Row Count: {df_2015_processed.count()}\")\n",
    "\n",
    "\n",
    "# Cell 5: Load Saved Data and Union\n",
    "print(\"Loading processed data from Parquet and uniting...\")\n",
    "df_2013_reloaded = spark.read.parquet(output_path_2013)\n",
    "df_2015_reloaded = spark.read.parquet(output_path_2015)\n",
    "\n",
    "merged_df = df_2013_reloaded.unionByName(df_2015_reloaded)\n",
    "print(f\"Cleaned DataFrames merged. Total rows: {merged_df.count()}\")\n",
    "print(\"Schema of merged DataFrame:\")\n",
    "merged_df.printSchema()\n",
    "\n",
    "\n",
    "# Cell 6: Perform Class Balancing on the Merged DataFrame (Downsampling)\n",
    "print(\"Performing class balancing...\")\n",
    "counts_spark = merged_df.groupBy('HeartDiseaseorAttack').count().collect()\n",
    "majority_count = 0\n",
    "minority_count = 0\n",
    "for row in counts_spark:\n",
    "    if row['HeartDiseaseorAttack'] == 0:\n",
    "        majority_count = row['count']\n",
    "    elif row['HeartDiseaseorAttack'] == 1:\n",
    "        minority_count = row['count']\n",
    "\n",
    "print(f\"Original class counts: HeartDiseaseorAttack=0: {majority_count}, HeartDiseaseorAttack=1: {minority_count}\")\n",
    "\n",
    "df_majority = merged_df.filter(col('HeartDiseaseorAttack') == 0)\n",
    "df_minority = merged_df.filter(col('HeartDiseaseorAttack') == 1)\n",
    "\n",
    "sampling_fraction = minority_count / majority_count if majority_count > 0 else 0\n",
    "\n",
    "df_majority_downsampled = df_majority.sample(False, sampling_fraction, seed=42)\n",
    "\n",
    "balanced_df = df_minority.union(df_majority_downsampled)\n",
    "\n",
    "balanced_df = balanced_df.orderBy(rand(seed=42))\n",
    "\n",
    "print(\"Class sizes after balancing:\")\n",
    "balanced_df.groupBy('HeartDiseaseorAttack').count().show()\n",
    "print(f\"Balanced DataFrame row count: {balanced_df.count()}\")\n",
    "\n",
    "\n",
    "# Cell 7: Dump Cleaned Dataset to MySQL\n",
    "print(\"Attempting to dump cleaned dataset to MySQL...\")\n",
    "jdbcHostname = \"127.0.0.1\"\n",
    "jdbcPort = 3306\n",
    "jdbcDatabase = \"test\"\n",
    "jdbcUsername = \"bigdata\"\n",
    "jdbcPassword = \"Bigdata@123\"\n",
    "jdbcTableName = \"heart_disease_data_balanced\"\n",
    "\n",
    "jdbcUrl = f\"jdbc:mysql://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?useSSL=false&allowPublicKeyRetrieval=true\"\n",
    "jdbcDriver = \"com.mysql.cj.jdbc.Driver\"\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\" : jdbcUsername,\n",
    "  \"password\" : jdbcPassword,\n",
    "  \"driver\" : jdbcDriver\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(f\"Writing data to MySQL table: {jdbcTableName} at {jdbcUrl.split('?')[0]}...\")\n",
    "    balanced_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbcUrl) \\\n",
    "        .option(\"dbtable\", jdbcTableName) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .options(**connectionProperties) \\\n",
    "        .save()\n",
    "    print(f\"Successfully dumped cleaned data to MySQL table: {jdbcTableName}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to MySQL: {e}\")\n",
    "\n",
    "# Cell 8: Stop Spark Session\n",
    "spark.stop()\n",
    "print(\"Spark Session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0980d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
